---
Last Modification: 2025-08-08 09:09
tags:
  - Sofware-Engineering
  - LLMs
  - Transformers
Theme: AI
---


## ðŸ“š Idea/Concept 
An LLM is a deep learning model based on large-scale neural networks, designed to process and generate text. Most LLMs use the Transformer architecture, which incorporates self-attention mechanisms.

Before applying attention, the text is converted into tokens, which are represented as vectors through embeddings and enriched with positional encodings, allowing the model to capture both their meaning and their order in the sequence.

During training, LLMs learn to predict the next token by adjusting parameters through backpropagation. This training consists of two phases: pretraining, which uses large amounts of unlabeled data to optimize token prediction, and fine-tuning, which employs more specific and sometimes labeled datasets, adapting the model to perform concrete tasks.
## ðŸ“Œ Key points (Optional)
- An LLM is a deep learning model based on large-scale neural networks, designed to process and generate text, most commonly using the Transformer architecture with self-attention.
- Text is first converted into tokens, which are represented as vectors through embeddings and enriched with positional encodings to capture both meaning and order.
- Training involves predicting the next token using backpropagation, carried out in two phases: pretraining on large amounts of unlabeled data, and fine-tuning on more specific datasets to adapt the model to concrete tasks.

## ðŸ”— Connections
- [[Attention mechanism in the Transformer architecture]]
- [[Context window of a model]]
- [[Neural network]]
- [[Tokenization]]
- [[Embedding]]

